# -*- coding: utf-8 -*-
"""Resnet_TFRecords_Testing_EVA3_A15.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1h8wWTgkDssaU2_jOEJ5zz2TTTpX2gtqi
"""

import numpy as np
import time, math
from tqdm import tqdm_notebook as tqdm

import tensorflow as tf
import tensorflow.contrib.eager as tfe
import matplotlib.pyplot as plt

tf.enable_eager_execution()

BATCH_SIZE = 512 #@param {type:"integer"}
MOMENTUM = 0.85 #@param {type:"number"}
LEARNING_RATE = 0.4 #@param {type:"number"}
WEIGHT_DECAY = 5e-4 #@param {type:"number"}
EPOCHS = 24 #@param {type:"integer"}

def init_pytorch(shape, dtype=tf.float32, partition_info=None):
  fan = np.prod(shape[:-1])
  bound = 1 / math.sqrt(fan)
  return tf.random.uniform(shape, minval=-bound, maxval=bound, dtype=dtype)

class ConvBN(tf.keras.Model):
  def __init__(self, c_out):
    super().__init__()
    self.conv = tf.keras.layers.Conv2D(filters=c_out, kernel_size=3, padding="SAME", kernel_initializer=init_pytorch, use_bias=False)
    self.bn = tf.keras.layers.BatchNormalization(momentum=0.9, epsilon=1e-5)

  def call(self, inputs):
    return tf.nn.relu(self.bn(self.conv(inputs)))

class ResBlk(tf.keras.Model):
  def __init__(self, c_out, pool, res = False):
    super().__init__()
    self.conv_bn = ConvBN(c_out)
    self.pool = pool
    self.res = res
    if self.res:
      self.res1 = ConvBN(c_out)
      self.res2 = ConvBN(c_out)

  def call(self, inputs):
    h = self.pool(self.conv_bn(inputs))
    if self.res:
      h = h + self.res2(self.res1(h))
    return h

class DavidNet(tf.keras.Model):
  def __init__(self, c=64, weight=0.125):
    super().__init__()
    pool = tf.keras.layers.MaxPooling2D()
    self.init_conv_bn = ConvBN(c)
    self.blk1 = ResBlk(c*2, pool, res = True)
    self.blk2 = ResBlk(c*4, pool)
    self.blk3 = ResBlk(c*8, pool, res = True)
    self.pool = tf.keras.layers.GlobalMaxPool2D()
    self.linear = tf.keras.layers.Dense(10, kernel_initializer=init_pytorch, use_bias=False)
    self.weight = weight

  def call(self, x, y):
    h = self.pool(self.blk3(self.blk2(self.blk1(self.init_conv_bn(x)))))
    h = self.linear(h) * self.weight
    ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=h, labels=y)
    loss = tf.reduce_sum(ce)
    correct = tf.reduce_sum(tf.cast(tf.math.equal(tf.argmax(h, axis = 1), y), tf.float32))
    return loss, correct

(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()
len_train, len_test = len(x_train), len(x_test)
y_train = y_train.astype('int64').reshape(len_train)
y_test = y_test.astype('int64').reshape(len_test)

train_mean = np.mean(x_train, axis=(0,1,2))
train_std = np.std(x_train, axis=(0,1,2))

normalize = lambda x: ((x - train_mean) / train_std).astype('float32') # todo: check here
pad4 = lambda x: np.pad(x, [(0, 0), (4, 4), (4, 4), (0, 0)], mode='reflect')

x_train = normalize(pad4(x_train))
x_test = normalize(x_test)

def _bytes_feature(value):
  """Returns a bytes_list from a string / byte."""
  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))

def _int64_feature(value):
  """Returns an int64_list from a bool / enum / int / uint."""
  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))

def tf_example_protocol(image, label):
    feature_dict = {
        'image': _bytes_feature(image.tostring()),
        'label': _int64_feature(label),
    }

    features = tf.train.Features(feature=feature_dict)
    tf_example = tf.train.Example(features=features)
    protocol = tf_example.SerializeToString()

    return protocol

def convert_protocol_to_image(protocol_message):
    feature_dict = {
        'image': tf.io.FixedLenFeature([], tf.string),
        'label': tf.io.FixedLenFeature([], tf.int64)
    }

    parsed_out = tf.io.parse_single_example(serialized=protocol_message,
                                            features=feature_dict)
    
    image = tf.io.decode_raw(parsed_out['image'], out_type=tf.float32)
    # image = tf.reshape(image, shape=[32,32,3])
    label = parsed_out["label"]
    # label = tf.cast(parsed_out["label"], tf.int32)
    # label = tf.one_hot(label, 10, dtype=tf.int32)
    
    return (image,label)
    
def convert_np_to_tfrecords(images, labels,filename=None):
    # Converts a numpy array into TFReocrds
    if filename == None:
        filename = 'dataset.tfrecords'
    with tf.io.TFRecordWriter(filename) as writer:
        for img,label in zip(images,labels):
          # numpy check
          # if isinstance(x, (np.ndarray, np.generic)) and isinstance(y, (np.ndarray, np.generic)):
				  #   record = serialize_function(x, y)
          # else:
          #   record = serialize_function(x.numpy(), y.numpy())
            protocol_message = tf_example_protocol(img,label)
            writer.write(protocol_message)

convert_np_to_tfrecords(x_train, y_train,filename='train.tfrecords')
convert_np_to_tfrecords(x_test, y_test, filename='test.tfrecords')

def input_foo(filenames, buffer_size = 8*1024, seed = 50, batch_size = 128):
  dataset = tf.data.TFRecordDataset(filenames=filenames, buffer_size=buffer_size)
  # dataset = dataset.apply(tf.data.experimental.shuffle_and_repeat(buffer_size=buffer_size,seed=seed))
  # dataset = dataset.apply(tf.data.experimental.map_and_batch(map_func=convert_proto_message_to_np,batch_size=batch_size,num_parallel_calls=tf.data.experimental.AUTOTUNE))
  dataset = dataset.map(convert_protocol_to_image)
  if filenames == 'test.tfrecords':
    dataset = dataset.map(lambda x, y: (tf.reshape(x,[32,32,3]), y))
  else:
    dataset = dataset.map(lambda x, y: (tf.reshape(x,[40,40,3]), y))
  dataset = dataset.apply(tf.data.experimental.prefetch_to_device('/GPU:0'))
  dataset = dataset.prefetch(4)
  return dataset

test_dataset = input_foo(filenames='test.tfrecords')
train_dataset = input_foo(filenames='train.tfrecords')

def replace_slice(input_: tf.Tensor, replacement, begin) -> tf.Tensor:
    inp_shape = tf.shape(input_)
    size = tf.shape(replacement)
    padding = tf.stack([begin, inp_shape - (begin + size)], axis=1)
    replacement_pad = tf.pad(replacement, padding)
    mask = tf.pad(tf.ones_like(replacement, dtype=tf.bool), padding)
    return tf.where(mask, replacement_pad, input_)


def get_cutout_eraser(minimum, maximum, area: int = 81, c: int = 3, min_aspect_ratio=0.5, max_aspect_ratio=1 / 0.5):
    sqrt_area = np.sqrt(area)

    def get_h_w(aspect_ratio):
        h = sqrt_area / aspect_ratio
        w = tf.math.round(area / h)
        h = tf.math.round(h)
        h = tf.cast(h, tf.int32)
        w = tf.cast(w, tf.int32)
        return h, w

    def tf_cutout(x: tf.Tensor) -> tf.Tensor:
        """
        Cutout data augmentation. Randomly cuts a h by w whole in the image, and fill the whole with zeros.
        :param x: Input image.
        :param h: Height of the hole.
        :param w: Width of the hole
        :param c: Number of color channels in the image. Default: 3 (RGB).
        :return: Transformed image.
        """
        p_1 = np.random.rand()
        if p_1 > 0.5:
          return x
        
        dtype = x.dtype
        minval = tf.cast(minimum, dtype=dtype)
        maxval = tf.cast(maximum, dtype=dtype)

        aspect_ratio = tf.random.uniform([], min_aspect_ratio, max_aspect_ratio)
        h, w = get_h_w(aspect_ratio)

        shape = tf.shape(x)
        x0 = tf.random.uniform([], 0, shape[0] + 1 - h, dtype=tf.int32)
        y0 = tf.random.uniform([], 0, shape[1] + 1 - w, dtype=tf.int32)

        # slic = tf.random.uniform([h, w, c], minval=minval, maxval=maxval, dtype=dtype)
        img_mean = tf.reduce_mean(x)
        
        # find cut portion mean instead of whole mean

        slic = tf.fill([h, w, c], img_mean)
        # print(tf.shape(slic))
        # plt.imshow(slic)
        # plt.show()
        
        x = replace_slice(x, slic, [x0, y0, 0])
        return x

    return tf_cutout
    
cutout = get_cutout_eraser(0,255)

lr = []
ep = []
trainAcc = []
testAcc = []
mt = []

model = DavidNet()
batches_per_epoch = len_train//BATCH_SIZE + 1


schedule = np.interp(np.arange(EPOCHS+1), [0, int((EPOCHS+1)*0.2), int((EPOCHS+1)*0.76), EPOCHS], [LEARNING_RATE*0.15, LEARNING_RATE, LEARNING_RATE*0.05, 0.001]) 
lr_schedule = lambda t: schedule[t]
# lr_schedule = lambda t: np.interp([t], [0, 5, 6, 12, 16, 19, 22, 24], [0.03, 0.6, 0.58, 0.387, 0.2, 0.1, 0.01, 0.001])[0]
# lr_schedule = lambda t: np.interp([t], [0,     5,    6,     12,     16,  19, 22, 24], 
#                                          [0.03,  0.6, 0.58,  0.387,  0.2,  0.1,0.01,0.001])[0]

m_schedule = np.interp(np.arange(EPOCHS+1), [0, int((EPOCHS+1)*0.2), int((EPOCHS+1)*0.76), EPOCHS], [MOMENTUM*1.01, MOMENTUM*0.91, MOMENTUM, MOMENTUM]) 
moment_schedule = lambda mt: m_schedule[mt]



# lr_schedule = lambda t: np.interp([t], [0, (EPOCHS+1)//5, EPOCHS], [0, LEARNING_RATE, 0])[0]
global_step = tf.train.get_or_create_global_step()

lr_func = lambda: lr_schedule(global_step/batches_per_epoch)/BATCH_SIZE
moment_func = lambda: moment_schedule(global_step/batches_per_epoch)/BATCH_SIZE

opt = tf.train.MomentumOptimizer(lr_func, momentum=moment_func, use_nesterov=True)
# opt = tf.train.MomentumOptimizer(lr_func, momentum=MOMENTUM, use_nesterov=True)

data_aug = lambda x, y: (cutout(tf.image.random_flip_left_right(tf.random_crop(x, [32, 32, 3]))), y)

t = time.time()
test_set = test_dataset.batch(BATCH_SIZE)

for epoch in range(EPOCHS):

  lr_ep = lr_schedule(epoch+1)
  lr.append(lr_ep)
  mt_ep = moment_schedule(epoch+1)
  mt.append(mt_ep)

  train_loss = test_loss = train_acc = test_acc = 0.0
  train_set = train_dataset.map(data_aug).shuffle(len_train).batch(BATCH_SIZE).prefetch(1)

  tf.keras.backend.set_learning_phase(1)
  for (x, y) in tqdm(train_set):
    with tf.GradientTape() as tape:
      loss, correct = model(x, y)

    var = model.trainable_variables
    grads = tape.gradient(loss, var)
    for g, v in zip(grads, var):
      g += v * WEIGHT_DECAY * BATCH_SIZE
    opt.apply_gradients(zip(grads, var), global_step=global_step)

    train_loss += loss.numpy()
    train_acc += correct.numpy()

  tf.keras.backend.set_learning_phase(0)
  for (x, y) in test_set:
    loss, correct = model(x, y)
    test_loss += loss.numpy()
    test_acc += correct.numpy()
  
  testAcc.append(test_acc / len_test)
  trainAcc.append(train_acc / len_train)
  ep.append(epoch+1)  
  print('epoch:', epoch+1, 'lr:', lr_schedule(epoch+1), 'train loss:', train_loss / len_train, 'train acc:', train_acc / len_train, 'val loss:', test_loss / len_test, 'val acc:', test_acc / len_test, 'time:', time.time() - t)

plt.plot(ep, lr)

plt.plot(ep, mt)

plt.plot(ep, testAcc)

plt.plot(ep, trainAcc)

